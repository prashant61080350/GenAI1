# Backend selection: mock | local | hf_api | ollama
MODEL_BACKEND=ollama

# Ollama settings (install and run Ollama separately; pull a model, e.g., `ollama pull llama3.2:1b`)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2:1b

# Local Transformers backend (requires transformers + torch)
MODEL_NAME=TinyLlama/TinyLlama-1.1B-Chat-v1.0

# Hugging Face Inference API backend
HF_API_MODEL=mistralai/Mistral-7B-Instruct-v0.2
HF_API_TOKEN=hf_xxx_your_token_here

# Logging
LOG_LEVEL=INFO